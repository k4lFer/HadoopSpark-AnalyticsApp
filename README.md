# Analytics System - Hadoop + Spark + Flask

Sistema completo de analytics con Hadoop, Spark y Flask API para anÃ¡lisis de datos a gran escala con soporte para archivos CSV de hasta 5GB.

## ğŸ—ï¸ Arquitectura Actual

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dashboard     â”‚    â”‚   Flask API     â”‚    â”‚   Spark Cluster â”‚
â”‚   HTML/JS       â”‚â—„â”€â”€â–ºâ”‚   Backend       â”‚â—„â”€â”€â–ºâ”‚   + Hadoop      â”‚
â”‚   (Port 5000)   â”‚    â”‚   (Port 5000)   â”‚    â”‚   (Ports 8080+) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componentes:
- **Frontend**: Dashboard HTML con JavaScript vanilla
- **Backend**: Flask API modular con PySpark
- **Processing**: Apache Spark cluster (1 master + 2 workers)
- **Storage**: Hadoop HDFS para almacenamiento distribuido

## ğŸš€ Inicio RÃ¡pido

### Prerrequisitos
- Docker y Docker Compose
- 8GB+ RAM recomendado
- 10GB+ espacio en disco

### InstalaciÃ³n
```bash
# Clonar repositorio
git clone <tu-repo>
cd analytics-project

# Crear estructura de directorios y generar datos de prueba
make install

# O paso a paso:
make setup    # Crear directorios
make build    # Construir contenedores
make start    # Iniciar servicios
```

### Acceso
- **Dashboard**: http://localhost:5000
- **API REST**: http://localhost:5000/api/*
- **Spark UI**: http://localhost:8080
- **Hadoop UI**: http://localhost:9870
- **Worker 1 UI**: http://localhost:8081
- **Worker 2 UI**: http://localhost:8082

## ğŸ“ Estructura del Proyecto

```
analytics-project/
â”œâ”€â”€ ğŸ Backend (Python/Flask)
â”‚   â”œâ”€â”€ app.py                 # AplicaciÃ³n principal (50 lÃ­neas)
â”‚   â”œâ”€â”€ config.py              # ConfiguraciÃ³n centralizada
â”‚   â”œâ”€â”€ requirements.txt       # Dependencias Python
â”‚   â””â”€â”€ src/                   # CÃ³digo fuente modular
â”‚       â”œâ”€â”€ api/               # Endpoints REST (~200 lÃ­neas c/u)
â”‚       â”‚   â”œâ”€â”€ __init__.py    # Registro de blueprints
â”‚       â”‚   â”œâ”€â”€ health.py      # /api/health
â”‚       â”‚   â”œâ”€â”€ upload.py      # /api/upload
â”‚       â”‚   â”œâ”€â”€ query.py       # /api/query, /api/aggregate
â”‚       â”‚   â”œâ”€â”€ data.py        # /api/sample, /api/stats
â”‚       â”‚   â””â”€â”€ utils.py       # Utilidades API
â”‚       â”‚
â”‚       â”œâ”€â”€ core/              # LÃ³gica de negocio (~150 lÃ­neas c/u)
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ analytics_app.py    # Clase principal simplificada
â”‚       â”‚   â”œâ”€â”€ spark_manager.py    # GestiÃ³n Spark
â”‚       â”‚   â”œâ”€â”€ data_analyzer.py    # AnÃ¡lisis con PySpark
â”‚       â”‚   â””â”€â”€ query_engine.py     # Motor SQL
â”‚       â”‚
â”‚       â”œâ”€â”€ utils/             # Utilidades reutilizables
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ responses.py   # Respuestas JSON
â”‚       â”‚   â”œâ”€â”€ validators.py  # Validaciones
â”‚       â”‚   â””â”€â”€ exceptions.py  # Manejo errores
â”‚       â”‚
â”‚       â””â”€â”€ templates/         # Frontend actual
â”‚           â””â”€â”€ dashboard.html # Dashboard principal
â”‚
â”œâ”€â”€ ğŸ³ Docker
â”‚   â”œâ”€â”€ docker-compose.yml     # OrquestaciÃ³n servicios
â”‚   â”œâ”€â”€ Dockerfile.flask       # Imagen Python/Spark
â”‚   â”œâ”€â”€ hadoop.env             # Variables Hadoop
â”‚   â””â”€â”€ spark-conf/
â”‚       â””â”€â”€ spark-defaults.conf # ConfiguraciÃ³n Spark
â”‚
â””â”€â”€ ğŸ› ï¸ Herramientas
    â”œâ”€â”€ Makefile               # Comandos automatizados
    â””â”€â”€ README.md              # Este archivo
```

## ğŸ”§ API Endpoints

### ğŸ“¤ GestiÃ³n de Datos
```bash
# Subir archivo CSV (hasta 5GB)
POST /api/upload
Content-Type: multipart/form-data
Body: file=archivo.csv

# Obtener muestra de datos
GET /api/sample?size=100

# EstadÃ­sticas del dataset
GET /api/stats

# InformaciÃ³n detallada de columnas
GET /api/columns
```

### ğŸ” Consultas SQL
```bash
# Ejecutar consulta SQL personalizada
POST /api/query
Content-Type: application/json
Body: {"query": "SELECT * FROM data LIMIT 10", "limit": 1000}

# Realizar agregaciones
POST /api/aggregate
Body: {"group_by": "column", "agg_function": "count"}

# Consultas de ejemplo
GET /api/queries/examples
```

### ğŸ”§ Sistema
```bash
# Estado del sistema
GET /api/health

# Insights automÃ¡ticos
GET /api/insights
```

## ğŸ’» Desarrollo Local

### Backend (Flask)
```bash
# Instalar dependencias
pip install -r requirements.txt

# Desarrollo sin Docker (Spark local)
export SPARK_MASTER_URL=local[*]
python app.py
```

### Arquitectura Modular del Backend

#### ğŸ”Œ `src/api/` - Endpoints REST
- **`health.py`** - Health checks y estado del sistema
- **`upload.py`** - Carga de archivos CSV a HDFS
- **`query.py`** - Consultas SQL y agregaciones
- **`data.py`** - GestiÃ³n de datos y estadÃ­sticas
- **`utils.py`** - Re-exports desde utils.responses

#### âš™ï¸ `src/core/` - LÃ³gica de Negocio
- **`analytics_app.py`** - AplicaciÃ³n principal (orchestrator)
- **`spark_manager.py`** - GestiÃ³n de sesiones Spark
- **`data_analyzer.py`** - AnÃ¡lisis de CSV con PySpark
- **`query_engine.py`** - Motor de consultas SQL

#### ğŸ› ï¸ `src/utils/` - Utilidades
- **`responses.py`** - Respuestas JSON seguras
- **`validators.py`** - Validaciones de entrada
- **`exceptions.py`** - Manejo de errores personalizado

## ğŸ› ï¸ Comandos Make

### GestiÃ³n del Sistema
```bash
make install          # ğŸ“¦ InstalaciÃ³n completa
make start            # ğŸš€ Iniciar servicios
make stop             # ğŸ›‘ Detener servicios
make restart          # ğŸ”„ Reiniciar servicios
make status           # ğŸ“Š Estado de servicios
```

### Desarrollo
```bash
make dev-start        # ğŸƒ Inicio rÃ¡pido desarrollo
make dev-restart      # âš¡ Reinicio rÃ¡pido Flask
make build            # ğŸ”¨ Construir contenedores
```

### Monitoreo y Debugging
```bash
make logs             # ğŸ“ Ver todos los logs
make logs-flask       # ğŸ Logs de Flask API
make logs-spark       # âš¡ Logs de Spark
make logs-hadoop      # ğŸ˜ Logs de Hadoop
make monitor          # ğŸ“ˆ Monitoreo de recursos
```

### Mantenimiento
```bash
make clean            # ğŸ§¹ Limpiar contenedores
make clean-data       # ğŸ—‘ï¸ Limpiar datos HDFS
make backup           # ğŸ’¾ Backup de datos
make test-api         # ğŸ§ª Probar endpoints
```

### Shell de Debugging
```bash
make shell-flask      # ğŸ Shell del contenedor Flask
make shell-spark      # âš¡ Shell del master Spark
```

## ğŸ“Š ConfiguraciÃ³n de Recursos

### LÃ­mites por Contenedor
| Contenedor | RAM | CPU | DescripciÃ³n |
|------------|-----|-----|-------------|
| Flask API | 4GB | 2 | Backend principal |
| Spark Master | 1GB | 1 | Coordinador Spark |
| Spark Worker 1 | 1.2GB | 2 | Procesador datos |
| Spark Worker 2 | 1.2GB | 2 | Procesador datos |
| Hadoop NameNode | - | - | Metadatos HDFS |
| Hadoop DataNode | - | - | AlmacÃ©n datos |

### ConfiguraciÃ³n Spark Optimizada
- **Workers**: 2 workers con 2 cores cada uno
- **Memoria**: 1GB por executor + 2GB driver
- **Particiones**: 4 particiones por defecto
- **SerializaciÃ³n**: JavaSerializer (estabilidad)
- **LÃ­mites**: Archivos hasta 5GB

## ğŸ”’ Seguridad y Validaciones

### Validaciones de Entrada
- âœ… Solo archivos CSV (extensiÃ³n verificada)
- âœ… TamaÃ±o mÃ¡ximo 5GB por archivo
- âœ… Consultas SQL solo de lectura (SELECT/WITH)
- âœ… ValidaciÃ³n de nombres de columnas
- âœ… ParÃ¡metros de consulta sanitizados

### Seguridad SQL
```python
# âŒ Bloqueadas automÃ¡ticamente:
DROP TABLE data;
DELETE FROM data;
INSERT INTO data VALUES...;

# âœ… Permitidas:
SELECT * FROM data WHERE column > 100;
WITH temp AS (SELECT...) SELECT * FROM temp;
```

## ğŸ“ˆ Monitoreo y Observabilidad

### URLs de Monitoreo
- **Spark Master**: http://localhost:8080
  - Estado del cluster
  - Aplicaciones activas
  - Workers conectados

- **Hadoop HDFS**: http://localhost:9870
  - Espacio utilizado
  - Archivos almacenados
  - Estado de DataNodes

- **Workers Spark**:
  - Worker 1: http://localhost:8081
  - Worker 2: http://localhost:8082

### Logs y Debugging
```bash
# Ver logs en tiempo real
docker-compose logs -f [servicio]

# Verificar salud del sistema
curl http://localhost:5000/api/health | jq

# MÃ©tricas de recursos
docker stats

# Estado de servicios
make status
```

## ğŸ› Troubleshooting

### Problemas Comunes

#### âŒ Spark no inicia
```bash
# Verificar memoria disponible
free -h

# Reiniciar cluster Spark
make restart
```

#### âŒ Error "No hay datos cargados"
```bash
# Verificar que el archivo se subiÃ³ correctamente
curl http://localhost:5000/api/stats

# Ver logs de upload
make logs-flask | grep upload
```

#### âŒ Consulta SQL falla
```bash
# Verificar sintaxis SQL
# Solo SELECT y WITH permitidos
# Tabla siempre se llama 'data'

# Ejemplo correcto:
SELECT column_name, COUNT(*) 
FROM data 
GROUP BY column_name 
LIMIT 100
```

#### âŒ Memoria insuficiente
```bash
# Aumentar memoria en docker-compose.yml
memory: 6G  # Para Flask API

# O reducir tamaÃ±o de archivo
# Archivos > 2GB pueden necesitar mÃ¡s memoria
```

### Logs de Debug
```bash
# Logs detallados por servicio
make logs-flask    # Backend Flask
make logs-spark    # Cluster Spark  
make logs-hadoop   # Sistema HDFS

# Verificar configuraciÃ³n Spark
make config-check
```

## ğŸ“Š Ejemplos de Uso

### 1. Cargar y Analizar Datos
```bash
# 1. Ir a http://localhost:5000
# 2. SecciÃ³n "Cargar Datos" 
# 3. Seleccionar archivo CSV
# 4. Click "Cargar Dataset"
# 5. Ver estadÃ­sticas automÃ¡ticas
```

### 2. Consultas SQL de Ejemplo
```sql
-- Conteo total
SELECT COUNT(*) as total_records FROM data;

-- Top categorÃ­as
SELECT category, COUNT(*) as count 
FROM data 
GROUP BY category 
ORDER BY count DESC 
LIMIT 10;

-- EstadÃ­sticas por grupo
SELECT region, 
       AVG(price) as avg_price,
       SUM(quantity) as total_quantity
FROM data 
GROUP BY region;

-- Filtros complejos
SELECT * FROM data 
WHERE price > 100 
  AND category = 'Electronics'
  AND quantity > 5
ORDER BY price DESC 
LIMIT 50;
```

### 3. Agregaciones RÃ¡pidas
```bash
# Via interfaz web:
# 1. SecciÃ³n "Agregaciones"
# 2. Agrupar por: "category" 
# 3. FunciÃ³n: "count"
# 4. Click "Calcular"

# Via API:
curl -X POST http://localhost:5000/api/aggregate \
  -H "Content-Type: application/json" \
  -d '{"group_by": "category", "agg_function": "count"}'
```

## ğŸ”„ PrÃ³ximas Mejoras

### ğŸ¯ Roadmap de Desarrollo

#### ğŸ“± Frontend Moderno (PrÃ³xima versiÃ³n)
- **Next.js 13+** con App Router
- **TypeScript** para type safety
- **Tailwind CSS** para styling
- **React Query** para estado de servidor
- **Componentes reutilizables** para visualizaciÃ³n

#### ğŸ“Š Visualizaciones Avanzadas
- **Charts.js/D3.js** para grÃ¡ficos interactivos
- **Plotly** para visualizaciones cientÃ­ficas  
- **Dashboard configurable** con widgets
- **ExportaciÃ³n** de grÃ¡ficos (PNG, PDF)

### ğŸ—ï¸ ReestructuraciÃ³n para Frontend Separado

Cuando agreguemos Next.js, la estructura serÃ¡:

```
analytics-project/
â”œâ”€â”€ ğŸŒ frontend/              # Frontend Next.js
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/             # App Router Next.js 13+
â”‚   â”‚   â”œâ”€â”€ components/      # Componentes React
â”‚   â”‚   â”œâ”€â”€ lib/            # Utilidades y API client
â”‚   â”‚   â””â”€â”€ hooks/          # Custom hooks
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ next.config.js
â”‚   â””â”€â”€ tailwind.config.js
â”‚
â”œâ”€â”€ ğŸ backend/              # Backend Flask (actual src/)
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ config.py  
â”‚   â””â”€â”€ src/                # CÃ³digo actual
â”‚
â”œâ”€â”€ ğŸ³ infra/               # Infraestructura
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ nginx.conf          # Reverse proxy
â”‚   â”œâ”€â”€ Dockerfile.flask
â”‚   â””â”€â”€ Dockerfile.nextjs
â”‚
â””â”€â”€ ğŸ“š docs/               # DocumentaciÃ³n
    â”œâ”€â”€ api.md
    â”œâ”€â”€ deployment.md
    â””â”€â”€ development.md
```

## ğŸ¤ Contribuir

1. Fork del proyecto
2. Crear rama feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit cambios (`git commit -am 'Agregar nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Crear Pull Request

### EstÃ¡ndares de CÃ³digo
- **Python**: PEP 8, type hints, docstrings
- **JavaScript**: ES6+, funciones puras
- **SQL**: Uppercase keywords, snake_case
- **Docker**: Multi-stage builds, .dockerignore

## ğŸ“„ Licencia


## ğŸ“ Soporte

- ğŸ› **Issues**: Reportar bugs y solicitar features
- ğŸ“– **Docs**: DocumentaciÃ³n detallada en `/docs`
- ğŸ’¬ **Discussions**: Preguntas y discusiones

---

## âš¡ Quick Start Commands

```bash
# InstalaciÃ³n completa en 3 comandos
git clone https://github.com/tu-usuario/HadoopSpark-AnalyticsApp.git
cd HadoopSpark-AnalyticsApp
make install

# Desarrollo rÃ¡pido
make dev-start

# Ver estado
make status

# Debugging
make logs-flask
```

**Â¡Sistema listo en http://localhost:5000!** ğŸ‰