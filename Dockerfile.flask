FROM python:3.9-slim-bullseye

WORKDIR /app

# Instalar Java 11 (disponible en Debian Bullseye)
RUN apt-get update && \
    apt-get install -y openjdk-11-jre-headless openjdk-11-jdk-headless wget curl && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Configurar variables de entorno PARA JAVA 11
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
ENV SPARK_VERSION=3.1.1
ENV HADOOP_VERSION=3.2
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
ENV PYSPARK_PYTHON=/usr/local/bin/python
ENV PYSPARK_DRIVER_PYTHON=/usr/local/bin/python
ENV PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9-src.zip:$PYTHONPATH

# Descargar e instalar Spark
RUN wget -q "https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" && \
    tar xzf "spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" && \
    mv "spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}" /opt/spark && \
    rm "spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz"

# Copiar requirements
COPY requirements.txt .

# Instalar dependencias Python
RUN pip install --no-cache-dir -r requirements.txt

# Crear directorios necesarios
RUN mkdir -p /app/data /app/src /app/templates /app/static

# Copiar todo el código de la aplicación
COPY requirements.txt .
COPY src/ ./src/
COPY templates/ ./templates/
COPY *.py ./

# Asegurar que los directorios existen
RUN mkdir -p /app/src /app/templates

EXPOSE 5000

CMD ["python", "app.py"]